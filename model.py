"""
Ovis ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ê´€ë ¨ í•¨ìˆ˜ë“¤
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import os

class OvisModel:
    def __init__(self, model_path="AIDC-AI/Ovis2-8B"):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.visual_tokenizer = None
        self.torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        
        # hf_cache ê²½ë¡œ ì„¤ì • (Ovis í´ë” ë‚´ì˜ hf_cache ì‚¬ìš©)
        self.cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "hf_cache")
        
    def load_model(self):
        """ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤"""
        print(f"Loading Ovis model from {self.model_path}...")
        print(f"Using cache directory: {self.cache_dir}")
        
        # cache ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.cache_dir):
            print(f"Warning: Cache directory {self.cache_dir} does not exist. Creating it...")
            os.makedirs(self.cache_dir, exist_ok=True)
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=self.torch_dtype,
            trust_remote_code=True,
            cache_dir=self.cache_dir,
            device_map="auto",
            low_cpu_mem_usage=True,
        )
        
        self.tokenizer = self.model.get_text_tokenizer()
        self.visual_tokenizer = self.model.get_visual_tokenizer()
        print("Model loaded successfully!")
        
    def analyze_image(self, image, text_prompt, max_tokens=1024, temperature=0.0, top_p=0.9):
        """ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
        if image is None:
            return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        if not text_prompt.strip():
            text_prompt = "ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        
        try:
            # PIL Imageë¡œ ë³€í™˜
            if hasattr(image, 'convert'):
                if image.mode != 'RGB':
                    image = image.convert('RGB')
            
            # ëª¨ë¸ ì¶”ë¡  ì¤€ë¹„
            images = [image]
            max_partition = 9
            query = f'<image>\n{text_prompt}'
            
            # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            prompt, input_ids, pixel_values = self.model.preprocess_inputs(
                query, images, max_partition=max_partition
            )
            attention_mask = torch.ne(input_ids, self.tokenizer.pad_token_id)
            input_ids = input_ids.unsqueeze(0).to(device=self.model.device)
            attention_mask = attention_mask.unsqueeze(0).to(device=self.model.device)
            
            if pixel_values is not None:
                pixel_values = pixel_values.to(
                    dtype=self.visual_tokenizer.dtype, 
                    device=self.visual_tokenizer.device
                )
            pixel_values = [pixel_values]
            
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
            gen_kwargs = {
                'max_new_tokens': int(max_tokens),
                'do_sample': temperature > 0,
                'top_p': top_p if temperature > 0 else None,
                'temperature': temperature if temperature > 0 else None,
                'repetition_penalty': 1.1,
                'eos_token_id': self.model.generation_config.eos_token_id,
                'pad_token_id': self.tokenizer.pad_token_id,
                'use_cache': True
            }
            
            # ì¶”ë¡  ì‹¤í–‰
            start_time = time.time()
            with torch.inference_mode():
                output_ids = self.model.generate(
                    input_ids, 
                    pixel_values=pixel_values, 
                    attention_mask=attention_mask, 
                    **gen_kwargs
                )[0]
                output = self.tokenizer.decode(output_ids, skip_special_tokens=True)
            end_time = time.time()
            
            # í† í° í†µê³„ ê³„ì‚°
            input_tokens = input_ids.shape[1]
            output_tokens = len(output_ids) - input_tokens
            total_tokens = len(output_ids)
            processing_time = end_time - start_time
            
            # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
            tokens_per_second = output_tokens / processing_time if processing_time > 0 else 0
            
            # ê²°ê³¼ ì •ë¦¬
            if query in output:
                result = output.replace(query, '').strip()
            else:
                result = output
            
            # ì²˜ë¦¬ í†µê³„ ì¶”ê°€
            stats_info = f"""
â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ
ğŸ”¢ ì…ë ¥ í† í°: {input_tokens:,}ê°œ
ğŸ“ ìƒì„± í† í°: {output_tokens:,}ê°œ
ğŸ“Š ì´ í† í°: {total_tokens:,}ê°œ
âš¡ ìƒì„± ì†ë„: {tokens_per_second:.1f} tokens/sec
ğŸ’¾ ì‚¬ìš© ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f}GB (GPU)"""
            
            result += stats_info
            
            return result
            
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ë°˜í™˜"""
        info = []
        info.append(f"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}")
        info.append(f"ğŸ® CUDA ì‚¬ìš© ê°€ëŠ¥: {'Yes' if torch.cuda.is_available() else 'No'}")
        
        if torch.cuda.is_available():
            info.append(f"ğŸ“Š GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_properties(i).name
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                info.append(f"  - GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        info.append(f"ğŸ’¾ ë°ì´í„° íƒ€ì…: {self.torch_dtype}")
        
        return "\n".join(info)
